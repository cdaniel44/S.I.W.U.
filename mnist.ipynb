{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape (60000, 784)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Sequential()\\nmodel.add(Dense(512, input_shape=(784,)))\\nmodel.add(Activation(\\'relu\\')) # An \"activation\" is just a non-linear function applied to the output\\n                              # of the layer above. Here, with a \"rectified linear unit\",\\n                              # we clamp all values below 0 to 0.\\n                           \\nmodel.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\\nmodel.add(Dense(512))\\nmodel.add(Activation(\\'relu\\'))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(10))\\nmodel.add(Activation(\\'softmax\\')) # This special \"softmax\" activation among other things,\\n                                 # ensures the output is a valid probaility distribution, that is\\n                                 # that its values are all non-negative and sum to 1.\\n                \\n# compilation du model ###############################################\\n\\nmodel.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])\\n\\n# training  ###########################################################\\n\\nmodel.fit(X_train, Y_train, batch_size=128, nb_epoch=4, verbose=1,\\n         validation_data=(X_test, Y_test))\\n         \\n# evualtion ###########################################################\\n         \\nscore = model.evaluate(X_test, Y_test, verbose=0)\\nprint(\\'Test score:\\', score[0])\\nprint(\\'Test accuracy:\\', score[1])\\n\\n#######################################################################\\n\\n# The predict_classes function outputs the highest probability class\\n# according to the trained classifier for each input example.\\npredicted_classes = model.predict_classes(X_test)\\n\\n# Check which items we got right / wrong\\ncorrect_indices = np.nonzero(predicted_classes == y_test)[0]\\nincorrect_indices = np.nonzero(predicted_classes != y_test)[0]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/local/bin/python3.6\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger , size of the figure\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#print(\"X_train original shape\", X_train.shape)\n",
    "#print(\"y_train original shape\", y_train.shape)\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))\n",
    "    #plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "#print(\"Training matrix shape\", X_train.shape)\n",
    "#print(X_train)\n",
    "#print(\"Testing matrix shape\", X_test.shape)\n",
    "\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# change une matrice de [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] en 5\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
    "                              # of the layer above. Here, with a \"rectified linear unit\",\n",
    "                              # we clamp all values below 0 to 0.\n",
    "                           \n",
    "model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
    "                                 # ensures the output is a valid probaility distribution, that is\n",
    "                                 # that its values are all non-negative and sum to 1.\n",
    "                \n",
    "# compilation du model ###############################################\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# training  ###########################################################\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=128, nb_epoch=4, verbose=1,\n",
    "         validation_data=(X_test, Y_test))\n",
    "         \n",
    "# evualtion ###########################################################\n",
    "         \n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "    plt.show()\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
